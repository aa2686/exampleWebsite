---
author: robjhyndman
comments: true
date: 2022-01-12
title: "Monash time series forecasting repository"
slug: forecastingdata
output: 
  html_document:
    keep_md: true
mathjax: true
categories:
  - time series
  - R
  - forecasting
  - data science
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
options(digits = 3, width = 75)
```

The [Monash time series forecasting respository](https://forecastingdata.org) is a comprehensive collection of time series data made available in a convenient form to encourage empirical forecast evaluations. The repository includes the data from many forecasting competitions including the M1, M3, M4, NN5, tourism, and KDD cup 2018, as well as many other data sets from diverse applications. The [associated paper](https://robjhyndman.com/publications/monash-forecasting-data/) discusses the various data sets and their characteristics. Where a time series collection contains data with different observation frequencies, they are split into different data sets so that the series within each data set has the same frequency.

In this post, I explain how to easily use these data sets with R. The function `monash_forecasting_respository()` from the `tsibbledata` package will download the data and return it as a [`tsibble` object](https://tsibble.tidyverts.org/articles/intro-tsibble.html). These can be analysed and plotted using the [`feasts` package](https://feasts.tidyverts.org), and modelled and forecast using the [`fable` package](https://fable.tidyverts.org). It is convenient to load these plus a few other packages via the `fpp3` package.

```{r packages, results="hide"}
library(fpp3)
```

(Note that the following code will not work with versions of `fabletools` and `tsibbledata` earlier than those listed above.)


# M3 benchmark

I will repeat a similar analysis to the benchmarks described at https://robjhyndman.com/hyndsight/benchmark-combination/ but using the `tsibbledata` and `fable` packages rather than the `Mcomp` and `forecast` packages.

To download the M3 data, we need to know the unique zenodo identifiers for each data set. From the [forecastingdata.org](https://forecastingdata.org) page, find the M3 links (there are four, one for each observational frequency). For example, the Yearly link takes you to https://zenodo.org/record/4656222, so the Zenodo identifier for this data set is 4656222. Similarly, the Quarterly, Monthly and Other links have identifiers 4656262, 4656298 and 4656335 respectively. 

```{r m3}
m3_yearly <- monash_forecasting_repository(4656222)
m3_quarterly <- monash_forecasting_repository(4656262)
m3_monthly <- monash_forecasting_repository(4656298)
m3_other <- monash_forecasting_repository(4656335)
```

The first three data sets are stored with a date index, so they are read as daily data. Therefore we first need to convert them to yearly, quarterly and monthly data.

```{r m3b}
m3_yearly <- m3_yearly %>%
  mutate(year = year(start_timestamp)) %>%
  as_tsibble(index=year) %>%
  select(-start_timestamp)
m3_quarterly <- m3_quarterly %>%
  mutate(quarter = yearquarter(start_timestamp)) %>%
  as_tsibble(index=quarter) %>%
  select(-start_timestamp)
m3_monthly <- m3_monthly %>%
  mutate(month = yearmonth(start_timestamp)) %>%
  as_tsibble(index=month) %>%
  select(-start_timestamp)
```

The resulting data sets are shown below.

```{r m3data}
m3_yearly
m3_quarterly
m3_monthly
m3_other
```

The M3 data included both training and test data. These have been combined in this data set. So to compare forecasting results against published methods, we need to filter out the test data. For the yearly and "other" time series, the test sets were six observations, while quarterly data had a test sets of eight observations and monthly data had test sets of eighteen observations.

For this benchmark analysis, we compare the Theta method (which won the M3 competition) against ETS and ARIMA models. The (seasonal) naive method is also included, along with an equally weighted combination forecast of ETS, ARIMA and Theta.

We will use the MASE and RMSSE forecast accuracy measures (as discussed at https://otexts.com/fpp3/accuracy.html). 

To avoid a lot of repetition, we will write a small function to produce the accuracy statistics for each tsibble.

```{r functions}
m3_accuracy <- function(data, horizon) {
  # Keep only training data
  training <- data %>%
    group_by(series_name) %>%
    slice(1:(n()-horizon)) %>%
    ungroup()
  # Produce forecasts
  forecasts <- training %>%
    model(
      N = NAIVE(value), T = THETA(value),
      E = ETS(value), A = ARIMA(value)
    ) %>%
    mutate(EAT = (E+A+T)/3) %>%
    forecast(h=horizon)
  # Compute and return accuracy
   forecasts %>% 
      group_by(.model, series_name) %>%
      mutate(h = row_number()) %>%
      ungroup() %>%
      accuracy(data) %>%
      group_by(.model) %>%
     summarise(MASE = mean(MASE))#, by=c(".model"))#,"h"))
}
```

Now we can produce a table of how well each method does for each observational period.

```{r m3accuracy, warning=FALSE}
accuracy_table <- bind_rows(
    m3_accuracy(m3_yearly, 6) %>% mutate(period = "Yearly"),
    #m3_accuracy(m3_quarterly, 8) %>% mutate(period = "Quarterly"),
    #m3_accuracy(m3_monthly, 18) %>% mutate(period = "Monthly"),
    #m3_accuracy(m3_other, 6) %>% mutate(period = "Other")
  ) 
accuracy_table %>%
  group_by(.model, period) %>%
  summarise(RMSSE = mean(RMSSE), MASE = mean(MASE), .groups="drop") %>%
  arrange(period, MASE)
```

And we can plot the accuracy statistics against the forecast horizon. 

```{r m3accuracygraphs}
accuracy_table %>%
  select(period, .model, h, RMSSE, MASE) %>%
  pivot_longer(RMSSE:MASE, values_to = "accuracy", names_to="measure") %>%
  mutate(
    period = factor(period, levels=c("Monthly","Quarterly","Yearly","Other")),
    .model = factor(.model, levels=c("N","T","A","E","EAT"))
    ) %>%
  ggplot(aes(x = h, y = accuracy), group = .model) +
    geom_line(aes(col = .model)) +
    facet_grid(measure ~ period, scale = "free") +
    xlab("Forecast horizon") + ylab("Forecast accuracy")
```
