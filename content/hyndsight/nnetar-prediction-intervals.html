---
date: 2017-05-25
slug: nnetar-prediction-intervals
title: Prediction intervals for NNETAR models
mathjax: true
categories:
- forecasting
- R
- time series
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>The <code>nnetar</code> function in the <a href="http://github.com/robjhyndman/forecast"><strong>forecast</strong> package for R</a> fits a neural network model to a time series with lagged values of the time series as inputs (and possibly some other exogenous inputs). So it is a nonlinear autogressive model, and it is not possible to analytically derive prediction intervals. Therefore we use simulation.</p>
<p>Suppose we fit a NNETAR model to the famous Canadian <code>lynx</code> data:</p>
<pre class="r"><code>library(forecast)
set.seed(2015)
(fit &lt;- nnetar(lynx, lambda=0.5))</code></pre>
<pre><code>## Series: lynx 
## Model:  NNAR(8,4) 
## Call:   nnetar(y = lynx, lambda = 0.5)
## 
## Average of 20 networks, each of which is
## a 8-4-1 network with 41 weights
## options were - linear output units 
## 
## sigma^2 estimated as 95.77</code></pre>
<p>I’ve used a Box-Cox transformation with <span class="math inline">\(\lambda=0.5\)</span> to ensure the residuals will be roughly homoscedastic.</p>
<p>The model can be written as
<span class="math display">\[
  y_t = f(\boldsymbol{y}_{t-1}) + \varepsilon_t
\]</span>
where <span class="math inline">\(\boldsymbol{y}_{t-1} = (y_{t-1},y_{t-2},\dots,y_{t-8})&#39;\)</span> is a vector containing lagged values of the series, and <span class="math inline">\(f\)</span> is a neural network with 4 hidden nodes in a single layer.</p>
<p>The error series <span class="math inline">\(\{\varepsilon_t\}\)</span> is assumed to be homoscedastic (and possibly also normally distributed).</p>
<p>We can simulate future sample paths of this model iteratively, by randomly generating a value for <span class="math inline">\(\varepsilon_t\)</span>, either from a normal distribution, or by resampling from the historical values. So if <span class="math inline">\(\varepsilon^*_{T+1}\)</span> is a random draw from the distribution of errors at time <span class="math inline">\(T+1\)</span>, then
<span class="math display">\[
  y^*_{T+1} = f(\boldsymbol{y}_{T}) + \varepsilon^*_{T+1}
\]</span>
is one possible draw from the forecast distribution for <span class="math inline">\(y_{T+1}\)</span>. Setting
<span class="math inline">\(\boldsymbol{y}_{T+1}^* = (y^*_{T+1}, y_{T}, \dots, y_{T-6})&#39;\)</span>, we can then repeat the process to get
<span class="math display">\[
  y^*_{T+2} = f(\boldsymbol{y}^*_{T+1}) + \varepsilon^*_{T+2}.
\]</span>
In this way, we can iteratively simulate a future sample path. By repeatedly simulating sample paths, we build up knowledge of the distribution for all future values based on the fitted neural network. Here is a simulation of 9 possible future sample paths for the lynx data. Each sample path covers the next 20 years after the observed data.</p>
<pre class="r"><code>sim &lt;- ts(matrix(0, nrow=20, ncol=9), start=end(lynx)[1]+1)
for(i in seq(9))
  sim[,i] &lt;- simulate(fit, nsim=20)

library(ggplot2)
autoplot(lynx) + forecast::autolayer(sim)</code></pre>
<p><img src="/hyndsight/nnetar-prediction-intervals_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>If we do this a few hundred or thousand times, we can get a very good picture of the forecast distributions. This is how the <code>forecast.nnetar</code> function produces prediction intervals:</p>
<pre class="r"><code>fcast &lt;- forecast(fit, PI=TRUE, h=20)
autoplot(fcast)</code></pre>
<p><img src="/hyndsight/nnetar-prediction-intervals_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Because it is a little slow, <code>PI=FALSE</code> is the default, so prediction intervals are not computed unless requested. The <code>npaths</code> argument in <code>forecast.nnetar</code> controls how many simulations are done (default 1000). By default, the errors are drawn from a normal distribution. The <code>bootstrap</code> argument allows the errors to be “bootstrapped” (i.e., randomly drawn from the historical errors).</p>
